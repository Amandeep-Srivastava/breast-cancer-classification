{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Linear Classification / Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#importing lib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LinearClassification(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        #defining hyperparams\n",
    "        self.learning_rate = 0.0001\n",
    "        self.batch_size = 200\n",
    "        self.no_of_iter = 1000\n",
    "        #videcu za ovaj\n",
    "        self.reg = 0.000001\n",
    "        \n",
    "    \n",
    "    #Input NOTE: X - matrix of data, can be used on images or numerical data (N x D)\n",
    "    #          N - Number of samples, D - Number of features\n",
    "    #          In case you use images make sure that X.shape[0] represent NUMBER of samples\n",
    "    #          y - labels (Nx1)\n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "        \n",
    "        #0 notation - so we add + 1 to max value from y\n",
    "        self.no_of_classes = np.max(y) + 1\n",
    "        \n",
    "        #defining hyperparams\n",
    "        # W - matrix of weights (No_of_classes x No_of_features)\n",
    "        self.W = np.random.rand(self.no_of_classes, self.X_train.shape[1]) * 0.001\n",
    "        \n",
    "        self.W, loss_history = self.SGD(self.W, self.X_train, self.y_train, self.learning_rate, self.batch_size, self.no_of_iter, self.reg)\n",
    "        \n",
    "        return loss_history\n",
    "    \n",
    "    #STOCHASTIC GRADIENT DESCENT\n",
    "    #Inputs: W - weights that we are trying to update\n",
    "    #        X - feautere of training set\n",
    "    #        y - wanted labels\n",
    "    #        learning_rate - how fast it is going to find good parameters\n",
    "    #        batch_size - how big PART of training set algo is using per iter\n",
    "    #        no_of_iter -  how many times it is going to run\n",
    "    #        reg - regularization\n",
    "    #\n",
    "    #Outputs: W_updated - updated weights matrix acording to loss function used\n",
    "    #         loss_history for verbose reptresentation of our loss computation\n",
    "    def SGD(self, W, X, y, learning_rate, batch_size, no_of_iter, reg):\n",
    "        W_updated = W\n",
    "        \n",
    "        no_of_train = X.shape[0]\n",
    "        #It is not necessities, but we can define loss_hitory to be sure that algo is working good\n",
    "        loss_history = []\n",
    "        \n",
    "        for i in range(no_of_iter):\n",
    "            batch_inx = np.random.choice(no_of_train, batch_size, replace=True)\n",
    "            #creting smallers train sets to fit in our SGD\n",
    "            X_batch = X[batch_inx,:]\n",
    "            y_batch = y[batch_inx]\n",
    "            \n",
    "            \n",
    "            loss, grad = self.SVM_classfier(W_updated, X_batch, y_batch, reg)\n",
    "            loss_history.append(loss)\n",
    "            #Update W:\n",
    "            W_updated = W_updated - (learning_rate * grad)\n",
    "            \n",
    "        return W_updated, loss_history\n",
    "            \n",
    "    #Inputs: W - current weights\n",
    "    #        X - training set features\n",
    "    #        y - training set labels\n",
    "    #        reg - regularization strenght\n",
    "    #\n",
    "    #Outputs: gradient_W - values to updated starting W\n",
    "    #         loss - to see if we are updaing in good direction\n",
    "    def SVM_classfier(self, W, X, y, reg):\n",
    "        \n",
    "        no_of_classes = np.max(y) + 1\n",
    "        #creating matrix with zeros, same shape as starting weights\n",
    "        \n",
    "        gradient_W = np.zeros(W.shape)\n",
    "        \n",
    "        loss = 0.0 \n",
    "        for i in range(X.shape[0]):\n",
    "            #First we need to multiply weights and x for particular sample\n",
    "            #need to transpose to long vector current sample\n",
    "            scores = W.dot(X[i, :].T)\n",
    "            #we are getting values for currect class\n",
    "            correct_class = scores[y[i]]\n",
    "            for j in range(no_of_classes):\n",
    "                if j == y[i]:\n",
    "                    continue\n",
    "                # This is simple formula for SVM\n",
    "                current_class_margin = scores[j] - correct_class + 1 #one is \n",
    "                if current_class_margin > 0:\n",
    "                    loss +=  current_class_margin\n",
    "                \n",
    "                    gradient_W[y[i]:1, :] -= X[i, :] #This is where we are creating gradient for CURRECT class\n",
    "                    gradient_W[j:1, :] += X[y[i], :]\n",
    "        \n",
    "        #average over number of train samples\n",
    "        loss /= X.shape[0]\n",
    "        gradient_W /= X.shape[0]\n",
    "        \n",
    "        loss += 0.5 * reg * np.sum(W * W)\n",
    "        \n",
    "        gradient_W += reg*W\n",
    "    \n",
    "        return loss, gradient_W\n",
    "    \n",
    "    #Predict function\n",
    "    #Input: X - test set \n",
    "    #\n",
    "    #Output: predict - list of classes\n",
    "    def predict(self, X):\n",
    "        pred = []\n",
    "        for i in range(X.shape[0]):\n",
    "            pred.append(np.argmax(np.dot(self.W,X[i, :].T)))\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#to check how much did algo predict right\n",
    "def accuracy(y_tes, y_pred):\n",
    "    correct = 0\n",
    "    for i in range(len(y_pred)):\n",
    "        if(y_tes[i] == y_pred[i]):\n",
    "            correct += 1\n",
    "    return (correct/len(y_tes))*100"
   ]
  },
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
